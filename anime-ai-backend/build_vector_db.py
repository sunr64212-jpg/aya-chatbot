import os
import glob
import shutil
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

# ================= é…ç½®åŒº =================
# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½• (å³ anime-ai-backend)
CURRENT_SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

# è·å–é¡¹ç›®æ ¹ç›®å½• (å³ chatbot1ï¼Œå‘ä¸Šä¸€çº§)
PROJECT_ROOT = os.path.dirname(CURRENT_SCRIPT_DIR)

# ä¿®æ­£ï¼šæŒ‡å‘éš”å£çš„ data_source ç›®å½•
DATA_SOURCE_DIR = os.path.join(PROJECT_ROOT, "data_source")

# æ•°æ®åº“ä¾ç„¶å­˜åœ¨å½“å‰è„šæœ¬ç›®å½•ä¸‹å³å¯
DB_PERSIST_DIR = os.path.join(CURRENT_SCRIPT_DIR, "chroma_db")
INDEX_MAP_FILE = os.path.join(DB_PERSIST_DIR, "index_map.txt")

EMBEDDING_MODEL_NAME = "shibing624/text2vec-base-chinese"


# =========================================

def extract_file_summary(file_path):
    """
    ä»æ–‡æœ¬æ–‡ä»¶å¤´éƒ¨æå–å…³é”®ä¿¡æ¯ï¼Œç”¨äºç”Ÿæˆè·¯ç”±ç´¢å¼•ã€‚
    è¯»å– [å…³é”®äººç‰©], [æ ¸å¿ƒäº‹ä»¶] æˆ– [ç®€ä»‹] ç­‰æ ‡ç­¾ã€‚
    """
    filename = os.path.basename(file_path)
    summary = f"- {filename}: "

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            # åªè¯»å‰ 10 è¡Œæ‰¾æ ‡ç­¾
            head_lines = [next(f) for _ in range(10)]

        tags = []
        for line in head_lines:
            line = line.strip()
            # æå–ä¸­æ‹¬å·å†…çš„ä¿¡æ¯ä½œä¸ºç®€ä»‹
            if line.startswith("[æ¡£æ¡ˆç±»å‹:") or line.startswith("[å‰§æƒ…é˜¶æ®µ:"):
                tags.append(line.split(":", 1)[1].strip(" ]"))
            elif line.startswith("[å…³é”®äººç‰©:") or line.startswith("[æ ¸å¿ƒäº‹ä»¶:"):
                content = line.split(":", 1)[1].strip(" ]")
                # æˆªå–è¿‡é•¿çš„æè¿°
                if len(content) > 20: content = content[:20] + "..."
                tags.append(content)

        if tags:
            summary += " / ".join(tags)
        else:
            # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œä½¿ç”¨é€šç”¨æè¿°ï¼ˆé’ˆå¯¹ 00_glossary ç­‰ï¼‰
            if "glossary" in filename:
                summary += "ä¸–ç•Œè§‚å®ä½“å­—å…¸ / è§’è‰²æ˜µç§°å¯¹ç…§è¡¨"
            else:
                summary += "å‰§æƒ…æ¡£æ¡ˆ"

    except Exception as e:
        summary += "æœªçŸ¥æ¡£æ¡ˆ"

    return summary


def process_memory_file(file_path):
    # ... (ä¿æŒåŸæœ¬çš„åˆ‡åˆ†é€»è¾‘ä¸å˜ï¼Œä¸ºäº†èŠ‚çœç¯‡å¹…çœç•¥) ...
    # è¿™é‡Œç›´æ¥å¤åˆ¶ä½ ä¹‹å‰ç¡®è®¤è¿‡çš„ process_memory_file å‡½æ•°å†…å®¹
    filename = os.path.basename(file_path)
    try:
        loader = TextLoader(file_path, encoding='utf-8')
        raw_docs = loader.load()
    except Exception:
        return []

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=600, chunk_overlap=150,
        separators=["\n\n", "\n", "ã€", "ã€‚", "ï¼", "ï¼Ÿ"]
    )
    docs = text_splitter.split_documents(raw_docs)

    final_docs = []
    for doc in docs:
        content = doc.page_content.strip()
        if not content: continue
        doc.page_content = f"ã€è®°å¿†æ¥æºï¼š{filename}ã€‘\n{content}"
        doc.metadata = {"source": filename, "category": "aya_memory"}
        final_docs.append(doc)
    return final_docs


def build_database():
    # 1. æ¸…ç†æ—§æ•°æ®
    if os.path.exists(DB_PERSIST_DIR):
        print(f"ğŸ—‘ï¸  æ­£åœ¨æ¸…ç†æ—§æ•°æ®åº“: {DB_PERSIST_DIR}")
        shutil.rmtree(DB_PERSIST_DIR)

    # å¿…é¡»é‡æ–°åˆ›å»ºç›®å½•ä»¥å­˜æ”¾ index_map.txt
    os.makedirs(DB_PERSIST_DIR, exist_ok=True)

    print(f"ğŸ“‚ å¼€å§‹æ‰«æè®°å¿†åº“: {DATA_SOURCE_DIR} ...")
    txt_files = glob.glob(os.path.join(DATA_SOURCE_DIR, "*.txt"))

    if not txt_files:
        print("âŒ ç›®å½•ä¸ºç©º")
        return

    all_docs = []
    index_lines = []  # ğŸ“ ç”¨äºå­˜å‚¨è·¯ç”±è¡¨å†…å®¹

    # 2. éå†å¤„ç†
    for txt_file in txt_files:
        filename = os.path.basename(txt_file)

        # A. ç”Ÿæˆç´¢å¼•æ¡ç›®
        summary_line = extract_file_summary(txt_file)
        index_lines.append(summary_line)

        # B. ç”Ÿæˆå‘é‡æ•°æ®
        docs = process_memory_file(txt_file)
        if docs:
            all_docs.extend(docs)
            print(f"   ğŸ“– å¤„ç†: {filename} -> {len(docs)} ç‰‡æ®µ | ç´¢å¼•: {summary_line}")

    # 3. ä¿å­˜è·¯ç”±ç´¢å¼•è¡¨åˆ° chroma_db æ–‡ä»¶å¤¹
    with open(INDEX_MAP_FILE, 'w', encoding='utf-8') as f:
        f.write("\n".join(sorted(index_lines)))
    print(f"ğŸ“ è·¯ç”±ç´¢å¼•è¡¨å·²ç”Ÿæˆ: {INDEX_MAP_FILE}")

    # 4. å‘é‡åŒ–å­˜åº“
    print(f"\nğŸš€ æ­£åœ¨å‘é‡åŒ– {len(all_docs)} æ¡æ•°æ®...")
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    Chroma.from_documents(
        documents=all_docs,
        embedding=embeddings,
        persist_directory=DB_PERSIST_DIR,
        collection_name="aya_memory_v3"
    )

    print(f"âœ… æ„å»ºå®Œæˆï¼æ•°æ®ä¸ç´¢å¼•å‡å·²ä¿å­˜è‡³ {DB_PERSIST_DIR}")


if __name__ == "__main__":
    build_database()