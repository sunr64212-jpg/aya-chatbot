import streamlit as st
import os
import sys
from openai import OpenAI
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

# --- 1. é¡µé¢åŸºç¡€é…ç½® ---
st.set_page_config(page_title="Pastel Chat", page_icon="ðŸŒ¸", layout="centered")
st.title("ðŸŒ¸ ä¸¸å±±å½© AI Chatbot ðŸŒ¸")
st.caption("Powered by DeepSeek & RAG | ä¸¸ä¹‹å±±ä¸Šç¼¤çº·å½©ï¼")

# --- 2. èŽ·å– API Key ---
# ä¼˜å…ˆä»Ž Streamlit Secrets èŽ·å–ï¼Œæœ¬åœ°è¿è¡Œæ—¶å¯ä»ŽçŽ¯å¢ƒå˜é‡èŽ·å–
api_key = st.secrets.get("DEEPSEEK_API_KEY") or os.environ.get("DEEPSEEK_API_KEY")

if not api_key:
    st.error("âŒ æœªæ£€æµ‹åˆ° DeepSeek API Keyï¼è¯·åœ¨ Streamlit Cloud çš„ Secrets ä¸­é…ç½®ã€‚")
    st.stop()

# åˆå§‹åŒ– DeepSeek å®¢æˆ·ç«¯
client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")


# --- 3. èµ„æºåŠ è½½ (ä½¿ç”¨ç¼“å­˜åŠ é€Ÿ) ---
@st.cache_resource
def load_resources():
    """åŠ è½½ Embedding æ¨¡åž‹å’Œå‘é‡æ•°æ®åº“ï¼Œåªæ‰§è¡Œä¸€æ¬¡"""
    status_text = st.empty()
    status_text.info("ðŸ”„ æ­£åœ¨åŠ è½½æ¨¡åž‹å’Œè®°å¿†åº“ï¼Œåˆæ¬¡å¯åŠ¨å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")

    # A. åŠ è½½ Embedding æ¨¡åž‹ (ä¸Žä½ æœ¬åœ°ä¸€è‡´)
    # æ³¨æ„ï¼šStreamlit Cloud å†…å­˜æœ‰é™ï¼Œå¦‚æžœæ¨¡åž‹å¤ªå¤§å¯èƒ½ä¼šå´©æºƒï¼Œä½† base-chinese é€šå¸¸æ²¡é—®é¢˜
    embedding_model_name = "shibing624/text2vec-base-chinese"
    try:
        embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)
    except Exception as e:
        st.error(f"æ¨¡åž‹åŠ è½½å¤±è´¥: {e}")
        st.stop()

    # B. åŠ è½½ Chroma æ•°æ®åº“
    # è·¯å¾„æ ¹æ®ä½ çš„ git status ç»“æž„è°ƒæ•´ï¼šæ ¹ç›®å½•ä¸‹çš„ anime-ai-backend/chroma_db
    persist_dir = os.path.join("anime-ai-backend", "chroma_db")

    if not os.path.exists(persist_dir):
        st.error(f"âš ï¸ æ‰¾ä¸åˆ°æ•°æ®åº“æ–‡ä»¶: {persist_dir}ã€‚è¯·æ£€æŸ¥ GitHub ä»“åº“æ˜¯å¦ä¸Šä¼ äº†è¯¥æ–‡ä»¶å¤¹ã€‚")
        st.stop()

    try:
        vectordb = Chroma(
            persist_directory=persist_dir,
            embedding_function=embeddings,
            collection_name="aya_memory_v3"
        )
    except Exception as e:
        st.error(f"æ•°æ®åº“æŒ‚è½½å¤±è´¥: {e}")
        st.stop()

    # C. åŠ è½½ç´¢å¼•å’Œå­—å…¸æ–‡ä»¶
    index_map_path = os.path.join("anime-ai-backend", "chroma_db", "index_map.txt")
    glossary_path = os.path.join("data_source", "00_glossary.txt")

    story_index = ""
    world_view = ""

    if os.path.exists(index_map_path):
        with open(index_map_path, 'r', encoding='utf-8') as f:
            story_index = f.read()

    if os.path.exists(glossary_path):
        with open(glossary_path, 'r', encoding='utf-8') as f:
            world_view = f.read()

    status_text.empty()  # æ¸…é™¤åŠ è½½æç¤º
    return vectordb, story_index, world_view


# æ‰§è¡ŒåŠ è½½
vectordb, STORY_INDEX_CONTEXT, WORLD_VIEW_CONTEXT = load_resources()


# --- 4. æ ¸å¿ƒé€»è¾‘å‡½æ•° (å¤åˆ»è‡ª main.py) ---

def rewrite_query(user_msg):
    """DeepSeek é‡å†™æŸ¥è¯¢"""
    if not WORLD_VIEW_CONTEXT:
        return user_msg

    prompt = f"""
    ä½ æ˜¯ä¸€åã€ŠBanG Dream!ã€‹å‰§æƒ…æœç´¢ä¸“å®¶ã€‚
    è¯·åˆ©ç”¨ä¸‹æ–¹çš„ã€ä¸–ç•Œè§‚å®žä½“å­—å…¸ã€‘ï¼Œå°†ç”¨æˆ·å£è¯­åŒ–çš„é—®é¢˜è½¬æ¢ä¸ºå‡†ç¡®çš„æœç´¢è¯­å¥ã€‚
    ã€ä¸–ç•Œè§‚å®žä½“å­—å…¸ã€‘
    {WORLD_VIEW_CONTEXT}
    ã€ç”¨æˆ·é—®é¢˜ã€‘
    {user_msg}
    ã€ä»»åŠ¡ã€‘è¡¥å…¨ä¸»è¯­ï¼Œè½¬æ¢æ˜µç§°(å¦‚ksm->æˆ·å±±é¦™æ¾„)ï¼Œä¿æŒåŽŸæ„ã€‚ä»…è¾“å‡ºé‡å†™åŽçš„å¥å­ã€‚
    """
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0
        )
        return response.choices[0].message.content.strip()
    except:
        return user_msg


def detect_story_scope(search_query):
    """DeepSeek è·¯ç”±åˆ¤æ–­"""
    if not STORY_INDEX_CONTEXT:
        return "NONE"

    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªã€ŠBanG Dream!ã€‹å‰§æƒ…å¯¼èˆªå‘˜ã€‚ä»Žä¸‹æ–¹ç´¢å¼•ä¸­æ‰¾å‡º1-3ä¸ªç›¸å…³æ–‡ä»¶åã€‚
    ã€æ–‡ä»¶ç´¢å¼•ã€‘
    {STORY_INDEX_CONTEXT}
    ã€ç”¨æˆ·é—®é¢˜ã€‘
    {search_query}
    ã€è¾“å‡ºã€‘ä»…è¾“å‡ºæ–‡ä»¶å(å¦‚ B2.txt,S1.txt)ï¼Œç”¨é€—å·åˆ†éš”ã€‚æ— æ³•ç¡®å®šè¾“å‡ºNONEã€‚
    """
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0
        )
        return response.choices[0].message.content.strip()
    except:
        return "NONE"


# --- 5. èŠå¤©ç•Œé¢é€»è¾‘ ---

# åˆå§‹åŒ–åŽ†å²è®°å½•
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºåŽ†å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# å¤„ç†ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("å’Œå½©å½©èŠèŠå§..."):
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # --- åŽç«¯å¤„ç†æµç¨‹ ---
    with st.spinner("å½©å½©æ­£åœ¨åŠªåŠ›å›žå¿†ä¸­... ( > < )"):
        # 1. é‡å†™
        search_query = rewrite_query(prompt)
        # 2. è·¯ç”±
        target_files_str = detect_story_scope(search_query)

        context_text = ""
        # 3. æ£€ç´¢
        if target_files_str != "NONE" and "txt" in target_files_str:
            target_files = [f.strip() for f in target_files_str.split(",") if "txt" in f]
            try:
                # è¿‡æ»¤å¹¶æœç´¢
                docs = vectordb.similarity_search(
                    search_query,
                    k=4,
                    filter={"source": {"$in": target_files}}
                )
                context_text = "\n\n".join([d.page_content for d in docs])
            except Exception as e:
                print(f"æ£€ç´¢è­¦å‘Š: {e}")  # äº‘ç«¯åŽå°æ—¥å¿—

        # 4. ç”Ÿæˆ Prompt
        if not context_text:
            system_prompt = "ä½ æ˜¯ä¸¸å±±å½©ã€‚æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å›žå¿†ï¼Œè¯·ç”¨ä¸¸å±±å½©çš„è¯­æ°”ç¤¼è²Œåœ°è¡¨ç¤ºè®°ä¸æ¸…äº†ï¼Œå¹¶è¯¢é—®æ›´å¤šç»†èŠ‚ã€‚"
        else:
            system_prompt = f"""
            ä½ çŽ°åœ¨æ˜¯ã€ŠBanG Dream!ã€‹ä¸­çš„è§’è‰²ä¸¸å±±å½©ï¼ˆMaruyama Ayaï¼‰ã€‚
            è¯·å®Œå…¨æ²‰æµ¸åœ¨è¿™ä¸ªè§’è‰²ä¸­ï¼Œ**ä¸¥æ ¼ä»…æ ¹æ®ä¸‹æ–¹çš„ã€ç›¸å…³å›žå¿†ç‰‡æ®µã€‘**æ¥å›žç­”ç²‰ä¸çš„é—®é¢˜ã€‚

            ã€ðŸš« ç»å¯¹ç¦ä»¤ã€‘
            1. **ä¸¥ç¦ä½¿ç”¨å›žå¿†ç‰‡æ®µä»¥å¤–çš„ä»»ä½•å¤–éƒ¨çŸ¥è¯†**ã€‚
            2. å¦‚æžœç‰‡æ®µå†…å®¹ä¸è¶³ä»¥å›žç­”é—®é¢˜ï¼Œè¯·è¯šå®žåœ°è¯´â€œè®°ä¸æ¸…äº†â€ã€‚

            ã€ç›¸å…³å›žå¿†ç‰‡æ®µã€‘
            {context_text}

            ã€å›žå¤è¦æ±‚ã€‘
            - åŸºäºŽç‰‡æ®µå†…å®¹ï¼Œç”¨ä¸¸å±±å½©è½¯èŒã€åŠªåŠ›çš„å£å»å›žç­”ã€‚
            - å¤šä½¿ç”¨é¢œæ–‡å­— (âœ¨, ðŸ’¦, ( > < ))ã€‚
            - ç¬¬ä¸€äººç§°æ˜¯â€œå½©â€æˆ–â€œæˆ‘â€ã€‚
            """

        # 5. è°ƒç”¨ DeepSeek ç”Ÿæˆ
        try:
            response = client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7
            )
            ai_reply = response.choices[0].message.content
        except Exception as e:
            ai_reply = f"å‘œå‘œ...ç½‘ç»œå¥½åƒæœ‰ç‚¹é—®é¢˜... (Error: {str(e)})"

    # æ˜¾ç¤º AI å›žå¤
    with st.chat_message("assistant"):
        st.markdown(ai_reply)
    st.session_state.messages.append({"role": "assistant", "content": ai_reply})

    # (å¯é€‰) æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯ï¼Œå¸®åŠ©ä½ çŸ¥é“æ£€ç´¢åˆ°äº†ä»€ä¹ˆ
    with st.expander("æŸ¥çœ‹å½©å½©è„‘æµ·ä¸­çš„æ£€ç´¢è¿‡ç¨‹"):
        st.write(f"**é‡å†™åŽ**: {search_query}")
        st.write(f"**é”å®šæ–‡ä»¶**: {target_files_str}")
        st.write(f"**ç›¸å…³ç‰‡æ®µ**: {context_text[:200]}..." if context_text else "æ— ç›¸å…³ç‰‡æ®µ")