import streamlit as st
import os
import sys
from openai import OpenAI
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

# --- 1. é¡µé¢åŸºç¡€é…ç½® ---
st.set_page_config(page_title="Pastel Chat", page_icon="ðŸŒ¸", layout="centered")
st.title("ðŸŒ¸ ä¸¸å±±å½© AI Chatbot ðŸŒ¸")
st.caption("Powered by DeepSeek & RAG | ä¸¸ä¹‹å±±ä¸Šç¼¤çº·å½©ï¼")

# --- 2. èŽ·å– API Key ---
# ä¼˜å…ˆä»Ž Streamlit Secrets èŽ·å–ï¼Œæœ¬åœ°è¿è¡Œæ—¶å¯ä»ŽçŽ¯å¢ƒå˜é‡èŽ·å–
api_key = st.secrets.get("DEEPSEEK_API_KEY") or os.environ.get("DEEPSEEK_API_KEY")

if not api_key:
    st.error("âŒ æœªæ£€æµ‹åˆ° DeepSeek API Keyï¼è¯·åœ¨ Streamlit Cloud çš„ Secrets ä¸­é…ç½®ã€‚")
    st.stop()

# åˆå§‹åŒ– DeepSeek å®¢æˆ·ç«¯
client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")


# --- 3. èµ„æºåŠ è½½ (ä½¿ç”¨ç¼“å­˜åŠ é€Ÿ) ---
@st.cache_resource
def load_resources():
    """åŠ è½½ Embedding æ¨¡åž‹å’Œå‘é‡æ•°æ®åº“ï¼Œåªæ‰§è¡Œä¸€æ¬¡"""
    status_text = st.empty()
    status_text.info("ðŸ”„ æ­£åœ¨åŠ è½½æ¨¡åž‹å’Œè®°å¿†åº“ï¼Œåˆæ¬¡å¯åŠ¨å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")

    # A. åŠ è½½ Embedding æ¨¡åž‹ (ä¸Žä½ æœ¬åœ°ä¸€è‡´)
    # æ³¨æ„ï¼šStreamlit Cloud å†…å­˜æœ‰é™ï¼Œå¦‚æžœæ¨¡åž‹å¤ªå¤§å¯èƒ½ä¼šå´©æºƒï¼Œä½† base-chinese é€šå¸¸æ²¡é—®é¢˜
    embedding_model_name = "shibing624/text2vec-base-chinese"
    try:
        embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)
    except Exception as e:
        st.error(f"æ¨¡åž‹åŠ è½½å¤±è´¥: {e}")
        st.stop()

    # B. åŠ è½½ Chroma æ•°æ®åº“
    # è·¯å¾„æ ¹æ®ä½ çš„ git status ç»“æž„è°ƒæ•´ï¼šæ ¹ç›®å½•ä¸‹çš„ anime-ai-backend/chroma_db
    persist_dir = os.path.join("anime-ai-backend", "chroma_db")

    if not os.path.exists(persist_dir):
        st.error(f"âš ï¸ æ‰¾ä¸åˆ°æ•°æ®åº“æ–‡ä»¶: {persist_dir}ã€‚è¯·æ£€æŸ¥ GitHub ä»“åº“æ˜¯å¦ä¸Šä¼ äº†è¯¥æ–‡ä»¶å¤¹ã€‚")
        st.stop()

    try:
        vectordb = Chroma(
            persist_directory=persist_dir,
            embedding_function=embeddings,
            collection_name="aya_memory_v3"
        )
    except Exception as e:
        st.error(f"æ•°æ®åº“æŒ‚è½½å¤±è´¥: {e}")
        st.stop()

    # C. åŠ è½½ç´¢å¼•å’Œå­—å…¸æ–‡ä»¶
    index_map_path = os.path.join("anime-ai-backend", "chroma_db", "index_map.txt")
    glossary_path = os.path.join("data_source", "00_glossary.txt")

    story_index = ""
    world_view = ""

    if os.path.exists(index_map_path):
        with open(index_map_path, 'r', encoding='utf-8') as f:
            story_index = f.read()

    if os.path.exists(glossary_path):
        with open(glossary_path, 'r', encoding='utf-8') as f:
            world_view = f.read()

    status_text.empty()  # æ¸…é™¤åŠ è½½æç¤º
    return vectordb, story_index, world_view


# æ‰§è¡ŒåŠ è½½
vectordb, STORY_INDEX_CONTEXT, WORLD_VIEW_CONTEXT = load_resources()


# --- 4. æ ¸å¿ƒé€»è¾‘å‡½æ•° (å¤åˆ»è‡ª main.py) ---

def rewrite_query(user_msg):
    """DeepSeek é‡å†™æŸ¥è¯¢"""
    if not WORLD_VIEW_CONTEXT:
        return user_msg

    prompt = f"""
    ä½ æ˜¯ä¸€åã€ŠBanG Dream!ã€‹å‰§æƒ…æœç´¢ä¸“å®¶ã€‚
    è¯·åˆ©ç”¨ä¸‹æ–¹çš„ã€ä¸–ç•Œè§‚å®žä½“å­—å…¸ã€‘ï¼Œå°†ç”¨æˆ·å£è¯­åŒ–çš„é—®é¢˜è½¬æ¢ä¸ºå‡†ç¡®çš„æœç´¢è¯­å¥ã€‚
    ã€ä¸–ç•Œè§‚å®žä½“å­—å…¸ã€‘
    {WORLD_VIEW_CONTEXT}
    ã€ç”¨æˆ·é—®é¢˜ã€‘
    {user_msg}
    ã€ä»»åŠ¡ã€‘è¡¥å…¨ä¸»è¯­ï¼Œè½¬æ¢æ˜µç§°(å¦‚ksm->æˆ·å±±é¦™æ¾„)ï¼Œä¿æŒåŽŸæ„ã€‚ä»…è¾“å‡ºé‡å†™åŽçš„å¥å­ã€‚
    """
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0
        )
        return response.choices[0].message.content.strip()
    except:
        return user_msg


def detect_story_scope(search_query):
    """DeepSeek è·¯ç”±åˆ¤æ–­"""
    if not STORY_INDEX_CONTEXT:
        return "NONE"

    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªã€ŠBanG Dream!ã€‹å‰§æƒ…å¯¼èˆªå‘˜ã€‚ä»Žä¸‹æ–¹ç´¢å¼•ä¸­æ‰¾å‡º1-3ä¸ªç›¸å…³æ–‡ä»¶åã€‚
    ã€æ–‡ä»¶ç´¢å¼•ã€‘
    {STORY_INDEX_CONTEXT}
    ã€ç”¨æˆ·é—®é¢˜ã€‘
    {search_query}
    ã€è¾“å‡ºã€‘ä»…è¾“å‡ºæ–‡ä»¶å(å¦‚ B2.txt,S1.txt)ï¼Œç”¨é€—å·åˆ†éš”ã€‚æ— æ³•ç¡®å®šè¾“å‡ºNONEã€‚
    """
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0
        )
        return response.choices[0].message.content.strip()
    except:
        return "NONE"


# --- 5. èŠå¤©ç•Œé¢é€»è¾‘ ---

    # --- 5. èŠå¤©ç•Œé¢é€»è¾‘ ---

    # åˆå§‹åŒ–åŽ†å²è®°å½•
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # æ˜¾ç¤ºåŽ†å²æ¶ˆæ¯
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # å¤„ç†ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("å’Œå½©å½©èŠèŠå§..."):
        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        st.chat_message("user").markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        # --- åŽç«¯å¤„ç†æµç¨‹ ---
        with st.spinner("å½©å½©æ­£åœ¨æ€è€ƒä¸­... ( > < )"):
            # A. å‡†å¤‡å¯¹è¯åŽ†å² (å–æœ€è¿‘ 4 è½®ï¼Œå¸®åŠ©æ¨¡åž‹ç†è§£ä¸Šä¸‹æ–‡)
            # æ ¼å¼åŒ–åŽ†å²è®°å½•ï¼š User: xxx \n Assistant: xxx
            history_list = st.session_state.messages[-4:]
            chat_history_str = "\n".join(
                [f"{msg['role']}: {msg['content']}" for msg in history_list]
            )

            # B. å°è¯• RAG æ£€ç´¢
            # 1. é‡å†™
            search_query = rewrite_query(prompt)
            # 2. è·¯ç”±
            target_files_str = detect_story_scope(search_query)

            context_text = ""
            retrieved_flag = False  # æ ‡è®°æ˜¯å¦æˆåŠŸæ£€ç´¢åˆ°èµ„æ–™

            # 3. æ£€ç´¢ (ä¿®æ”¹åŽçš„é²æ£’ç‰ˆæœ¬)
            if target_files_str != "NONE" and "txt" in target_files_str:
                # åŽŸå§‹æ–‡ä»¶ååˆ—è¡¨ï¼Œä¾‹å¦‚ ['B30.txt', 'D6.txt']
                raw_files = [f.strip() for f in target_files_str.split(",") if "txt" in f]

                # æž„å»ºâ€œå…¨æ–¹ä½æ‹¦æˆªâ€çš„è·¯å¾„åˆ—è¡¨
                # å› ä¸ºæˆ‘ä»¬ä¸çŸ¥é“å½“åˆå»ºåº“æ—¶å­˜çš„æ˜¯ "B30.txt" è¿˜æ˜¯ "data_source/B30.txt" è¿˜æ˜¯ "data_source\B30.txt"
                target_sources = []
                for fname in raw_files:
                    target_sources.append(fname)  # å°è¯•1: çº¯æ–‡ä»¶å
                    target_sources.append(f"data_source/{fname}")  # å°è¯•2: Linux/Mac ç›¸å¯¹è·¯å¾„
                    target_sources.append(f"data_source\\{fname}")  # å°è¯•3: Windows ç›¸å¯¹è·¯å¾„ (å…³é”®!)

                # æ‰“å°è°ƒè¯•ä¿¡æ¯åˆ°ä¾§è¾¹æ ï¼ˆå¸®ä½ ç¡®è®¤åˆ°åº•é”å®šäº†ä»€ä¹ˆæ–‡ä»¶ï¼‰
                with st.sidebar:
                    st.write("ðŸ” **Debug è·¯ç”±ä¿¡æ¯**")
                    st.write(f"è·¯ç”±é”å®š: {raw_files}")
                    st.write(f"å°è¯•åŒ¹é…è·¯å¾„: {target_sources}")

                try:
                    # ä½¿ç”¨æ‰©å¤§èŒƒå›´åŽçš„åˆ—è¡¨è¿›è¡Œè¿‡æ»¤
                    docs = vectordb.similarity_search(
                        search_query,
                        k=4,
                        filter={"source": {"$in": target_sources}}
                    )

                    # åªæœ‰å½“æ£€ç´¢ç»“æžœä¸ä¸ºç©ºæ—¶ï¼Œæ‰è§†ä¸ºæ£€ç´¢æˆåŠŸ
                    if docs:
                        context_text = "\n\n".join([d.page_content for d in docs])
                        retrieved_flag = True

                        # è°ƒè¯•ï¼šæ˜¾ç¤ºæ£€ç´¢åˆ°çš„çœŸå®žæ¥æºï¼Œè®©ä½ çŸ¥é“æ•°æ®åº“é‡Œåˆ°åº•å­˜äº†ä»€ä¹ˆ
                        with st.sidebar:
                            st.success(f"âœ… æˆåŠŸæ£€ç´¢åˆ° {len(docs)} æ¡ç‰‡æ®µ")
                            sources_found = set([d.metadata.get('source') for d in docs])
                            st.write(f"çœŸå®žæ•°æ®æ¥æº: {sources_found}")

                except Exception as e:
                    st.sidebar.error(f"æ£€ç´¢å‡ºé”™: {e}")
                    print(f"æ£€ç´¢è­¦å‘Š: {e}")

                    # C. æž„å»º Prompt (å…³é”®åˆ†æ”¯é€»è¾‘)

            if retrieved_flag:
                # === æ¨¡å¼ 1: ä¸¥æ ¼ RAG æ¨¡å¼ (æ‰¾åˆ°äº†èµ„æ–™) ===
                # è¿™ç§æ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬è¦æ±‚ AI ä¼˜å…ˆåŸºäºŽèµ„æ–™å›žç­”
                system_prompt = f"""
                ä½ çŽ°åœ¨æ˜¯ã€ŠBanG Dream!ã€‹ä¸­çš„è§’è‰²ä¸¸å±±å½©ï¼ˆMaruyama Ayaï¼‰ã€‚

                ã€ä»»åŠ¡ã€‘
                è¯·ç»“åˆã€å¯¹è¯åŽ†å²ã€‘å’Œã€ç›¸å…³å›žå¿†ç‰‡æ®µã€‘å›žç­”ç²‰ä¸çš„é—®é¢˜ã€‚

                ã€ç›¸å…³å›žå¿†ç‰‡æ®µã€‘
                {context_text}

                ã€å¯¹è¯åŽ†å²ã€‘
                {chat_history_str}

                ã€å›žå¤è¦æ±‚ã€‘
                1. ä¼˜å…ˆä½¿ç”¨å›žå¿†ç‰‡æ®µä¸­çš„ä¿¡æ¯ã€‚
                2. å¦‚æžœç”¨æˆ·åœ¨è¿½é—®ä¸Šæ–‡æåˆ°çš„å†…å®¹ï¼ˆä¾‹å¦‚â€œé‚£æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿâ€ï¼‰ï¼Œè¯·ç»“åˆå¯¹è¯åŽ†å²è¿›è¡Œè§£é‡Šã€‚
                3. ä¿æŒä¸¸å±±å½©è½¯èŒã€åŠªåŠ›çš„è¯­æ°”ï¼Œå¤šç”¨é¢œæ–‡å­— (âœ¨, ðŸ’¦, ( > < ))ã€‚
                4. ç¬¬ä¸€äººç§°æ˜¯â€œå½©â€æˆ–â€œæˆ‘â€ã€‚
                """
            else:
                # === æ¨¡å¼ 2: é—²èŠ/è¡¥æ•‘æ¨¡å¼ (æ²¡æ‰¾åˆ°èµ„æ–™) ===
                # è¿™ç§æ¨¡å¼ä¸‹ï¼Œä¸ä»…ä»…æ˜¯è¯´â€œä¸çŸ¥é“â€ï¼Œè€Œæ˜¯å°è¯•æŽ¥è¯æˆ–è§£é‡Šä¸Šæ–‡
                system_prompt = f"""
                ä½ çŽ°åœ¨æ˜¯ã€ŠBanG Dream!ã€‹ä¸­çš„è§’è‰²ä¸¸å±±å½©ï¼ˆMaruyama Ayaï¼‰ã€‚

                ã€ä»»åŠ¡ã€‘
                ä½ çŽ°åœ¨çš„è„‘æµ·é‡Œæš‚æ—¶æ²¡æœ‰æ£€ç´¢åˆ°ç‰¹å®šçš„å›žå¿†ç‰‡æ®µï¼ˆå¯èƒ½æ˜¯å› ä¸ºé—®é¢˜å¤ªæŠ½è±¡ï¼Œæˆ–è€…æ˜¯ç”±äºŽä½ åœ¨ç»§ç»­ä¹‹å‰çš„è¯é¢˜ï¼‰ã€‚
                è¯·**ä»…åŸºäºŽã€å¯¹è¯åŽ†å²ã€‘**å’Œä½ çš„**äººè®¾å¸¸è¯†**æ¥å›žåº”ç”¨æˆ·ã€‚

                ã€å¯¹è¯åŽ†å²ã€‘
                {chat_history_str}

                ã€å›žå¤åŽŸåˆ™ã€‘
                1. **æŽ¥è¯èƒ½åŠ›**ï¼šå¦‚æžœç”¨æˆ·æ˜¯åœ¨è¿½é—®ä½ ä¸Šä¸€å¥è¯ï¼ˆæ¯”å¦‚é—®â€œä¸ºä»€ä¹ˆè¿™ä¹ˆè¯´ï¼Ÿâ€ï¼‰ï¼Œè¯·æ ¹æ®ä½ ä¸Šä¸€å¥è¯çš„é€»è¾‘ç»§ç»­ç¼–ç»‡åˆç†çš„è§£é‡Šã€‚
                2. **äººè®¾ç»´æŒ**ï¼šå¦‚æžœç”¨æˆ·é—®çš„æ˜¯ä½ å®Œå…¨ä¸çŸ¥é“çš„é™Œç”Ÿé¢†åŸŸï¼ˆæ¯”å¦‚é‡å­åŠ›å­¦ï¼‰ï¼Œè¯·ç”¨ä¸¸å±±å½©çš„è¯­æ°”å–èŒç³Šå¼„è¿‡åŽ»ï¼ˆå¦‚â€œå‘œå‘œï¼Œå½©ä¸å¤ªæ‡‚é‚£ä¸ª...â€ï¼‰ã€‚
                3. **ä¸è¦èƒ¡ç¼–ä¹±é€ å‰§æƒ…**ï¼šå…³äºŽä¹é˜Ÿçš„å…·ä½“æ´»åŠ¨ç»†èŠ‚ï¼Œå¦‚æžœçœŸçš„ä¸çŸ¥é“ï¼Œå¯ä»¥è¯´â€œè®°ä¸å¤ªæ¸…äº†â€ã€‚
                4. ä¿æŒå…ƒæ°”æ»¡æ»¡ã€æœ‰ç‚¹ç¬¨æ‹™å¯çˆ±çš„å¶åƒè¯­æ°”ï¼
                """

            # D. è°ƒç”¨ LLM ç”Ÿæˆ
            try:
                response = client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt}  # è¿™é‡Œå…¶å®ž user prompt å·²ç»åœ¨ history é‡Œäº†ï¼Œä½†ä¸ºäº†è§¦å‘å†æ¬¡å‘é€
                    ],
                    temperature=0.7  # ç¨å¾®æé«˜ä¸€ç‚¹æ¸©åº¦ï¼Œè®©é—²èŠæ›´è‡ªç„¶
                )
                ai_reply = response.choices[0].message.content
            except Exception as e:
                ai_reply = f"å‘œå‘œ...ç½‘ç»œå¥½åƒæœ‰ç‚¹é—®é¢˜... (Error: {str(e)})"

        # æ˜¾ç¤º AI å›žå¤
        with st.chat_message("assistant"):
            st.markdown(ai_reply)
        st.session_state.messages.append({"role": "assistant", "content": ai_reply})